# Universal Weakly Supervised Segmentation by Pixel-to-Segment Contrastive Learning
By [Tsung-Wei Ke](https://www1.icsi.berkeley.edu/~twke/), [Jyh-Jing Hwang](https://jyhjinghwang.github.io/), and [Stella X. Yu](http://www1.icsi.berkeley.edu/~stellayu/)

<img align="center" img src="misc/main.png" width="720">

Weakly supervised segmentation is challenging as sparsely labeled pixels do not
provide sufﬁcient supervision: A semantic segment may contain multiple distinc-
tive regions whereas adjacent segments may appear similar. Common approaches
use the few labeled pixels in all training images to train a segmentation model,
and then propagate labels within each image based on visual or feature similarity.
Instead, we treat segmentation as a semi-supervised pixel-wise metric learning
problem, where pixels in different segments are mapped to distinctive features.
Naturally, our unlabeled pixels participate not only in data-driven grouping within
each image, but also in discriminative feature learning within and across images.
Our results on Pascal VOC and DensePose datasets demonstrate our substantial
accuracy gain on various forms of weak supervision including image-level tags,
bounding boxes, labeled points, and scribbles.


## Code Base
This release of code is based on [SegSort](https://github.com/jyhjinghwang/SegSort) in ICCV 2019.

## Prerequisites

1. Linux
2. Python3 (>=3.5)
3. Cuda >= 9.2 and Cudnn >= 7.6

## Required Python Packages

1. pytorch >= 1.6
2. numpy
3. scipy
4. tqdm
5. easydict == 1.9
6. PyYAML
7. PIL
8. opencv
9. pydensecrf

## Data Preparation

### Pascal VOC 2012

1. Augmented Pascal VOC training set by [SBD](http://home.bharathh.info/pubs/codes/SBD/download.html). [Download link](https://github.com/jyhjinghwang/SegSort/blob/master/dataset/voc12/sbd_clsimg.zip) provided by [SegSort](https://github.com/jyhjinghwang/SegSort).
2. Ground-truth semantic segmentation masks are reformatted as grayscale images. [Download link](https://www.dropbox.com/sh/fd2m7s87gk7jeyh/AAC6tN6syhFKmEaDYCwUIgnXa?dl=0) provided by [SegSort](https://github.com/jyhjinghwang/SegSort).
3. The over-segmentation masks are generated by combining contour detectors with gPb-owt-ucm. [HED-owt-ucm masks](https://www.dropbox.com/sh/fd2m7s87gk7jeyh/AAC6tN6syhFKmEaDYCwUIgnXa?dl=0) provided by [SegSort](https://github.com/jyhjinghwang/SegSort).
4. Scribble and point annotations by [ScribbleSup](https://jifengdai.org/downloads/scribble_sup/). You can download the processed ground-truth [scribble]() and [point]() annotations, and put them under VOC2012/scribble folder. For scribbles, we set dilation size to 3; For points, we set dilation size to 6.
5. Cam annotations by [SEAM](https://github.com/YudeWang/SEAM). You can download the processed CAMs for [image tag]() and [bounding box]() annotations, and put them under VOC2012/cam. For image tags, we set `alpha` to 6 and probability threshold to 0.2. For bounding boxes, we set `alpha` to 6, probability threshold to 0.5 and the pixel labels outside the boxes as background class.
6. Dataset layout:
```
   $DATAROOT/
       |-------- sbd/
       |          |-------- dataset/
       |                       |-------- clsimg/
       |
       |-------- VOC2012/
                  |-------- JPEGImages/
                  |-------- segcls/
                  |-------- hed/
                  |-------- scribble/
                  |            |-------- dilate_3/
                  |            |-------- dilate_6_0.0/
                  |
                  |-------- cam/
                               |-------- seam_a6_th0.2/
                               |-------- seambox_a6_th0.5/
```

## ImageNet Pre-Trained Models
ImageNet pretrained ResNet101 by [EMANet](https://github.com/XiaLiPKU/EMANet). You can download the pretrained models [here]() and put it under a new directory SPML/snapshots/imagenet/trained/. **Note: we do not use MSCOCO pretrained ResNet.**

## Pascal VOC 2012 Trained Models.
We provide the download links for our SPML models trained using image-level tag/bounding box/scribble/point annotations on PASCAL VOC, and summarize the performance as follows. **Note: we report the performance with denseCRF post-processing.**

| Annotations   |      val      |     test      |
| ------------- | ------------- | ------------- |
|  [Image Tags]()   |     69.5      |      ?        |
| [Bounding Box]()  |     73.5      |      ?        |
| [Scribbles]()  |     76.1      |      ?        |
| [Points]()  |     73.2     |      ?        |

## Bashscripts to Get Started

* SPML with image-level tag.
```
source bashscripts/voc12/train_spml_imagetag.sh
```

* SPML with bounding box.
```
source bashscripts/voc12/train_spml_bbox.sh
```

* SPML with scribbles.
```
source bashscripts/voc12/train_spml_scribble.sh
```

* SPML with points.
```
source bashscripts/voc12/train_spml_point.sh
```

## Citation

If you find this code useful for your research, please consider citing our paper Universal Weakly Supervised Segmentation by Pixel-to-Segment Contrastive Learning.
```
@inproceedings{,
  title={Universal Weakly Supervised Segmentation by Pixel-to-Segment Contrastive Learning},
  author={Ke, Tsung-Wei and Hwang, Jyh-Jing and Yu, Stella X},
  booktitle={},
  pages={},
  year={}
}
```

## License

SPML is released under the MIT License (refer to the LICENSE file for details).
